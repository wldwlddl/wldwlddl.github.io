<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[wldwlddl.github.io]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib\media\favicon.png</url><title>wldwlddl.github.io</title><link/></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Thu, 08 Aug 2024 08:43:15 GMT</lastBuildDate><atom:link href="lib\rss.xml" rel="self" type="application/rss+xml"/><pubDate>Thu, 08 Aug 2024 08:43:15 GMT</pubDate><ttl>60</ttl><dc:creator/><item><title><![CDATA[모각코 5회차 개인 목표 및 공부결과]]></title><description><![CDATA[ 
 <br><br>
<br>gan이 뭔지 알기
<br>실습을 위한 코드조사
<br><br><br>
<br>&nbsp;Generative Adversarial Networks(적대적 생성 신경망)의 약자로 생성 AI 모델 중 하나이다. 서로 다른 두 개의 네트워크를 적대적으로 학습시키며 실제 데이터와 비슷한 데이터를 생성해내는 모델이다.
<br>이때 gan은 Genaerator(생성기), discriminator(판별기)라는 서로 다른 2개의 네트워크로 이루어져 있다. 생성기는 진짜 데이터셋에 가까운 가짜 데이터를 생성하고 판별기는 주어지는 표본이 가짜인지 진짜인지를 결정하는 방식이다. gan의 목적은 실제 데이터의 분포에 가까운 데이터를 생성하는 것이다. 이러한 특성으로 gan은 레이블이 따로 없는 비지도 학습에 속한다. 
<br><br>
<br>아래는 인터넷에서 가져온 예시 gan 모델 코드이다. 나는 이미지 데이터셋을 사용했다. 
<br>!pip install imageio
#  imageio: 이미지와 동영상 파일을 읽고 쓰는 데 사용되는 파이썬 라이브러리
import torch
import torch.nn as nn # 파이토치의 신경망 모듈을 nn이라는 약어로 import하겠단 명령어
import torch.optim as optim # 최적화 알고리즘을 구현하는 패키지
from torchvision import datasets, transforms
from torchvision.utils import save_image
import matplotlib.pyplot as plt
import os
import numpy as np
복사<br># Function to set the seed for reproducibility
import random
def set_seed(seed_value=42):
&nbsp; &nbsp; """Set seed for reproducibility."""
&nbsp; &nbsp; np.random.seed(seed_value)
&nbsp; &nbsp; torch.manual_seed(seed_value)
&nbsp; &nbsp; torch.cuda.manual_seed(seed_value)
&nbsp; &nbsp; torch.cuda.manual_seed_all(seed_value) &nbsp;# if you are using multi-GPU.
&nbsp; &nbsp; random.seed(seed_value)
&nbsp; &nbsp; os.environ['PYTHONHASHSEED'] = str(seed_value)
&nbsp; &nbsp; # The below two lines are for deterministic algorithm behavior in CUDA
&nbsp; &nbsp; torch.backends.cudnn.deterministic = True
&nbsp; &nbsp; torch.backends.cudnn.benchmark = False

# Set the seed
set_seed()
복사<br>
<br>이 과정은 머신러닝이나 딥러닝에서 재현성(동일한 데이터와 코드로 동일한 결과를 얻는 것)을 보장하기 위해 시드(특정한 시작 숫자. 이를 이용해서 컴퓨터는 정해진 알고리즘에 따라 난수처럼 보이는 수열을 생성함)를 설정하는 함수이다.  위에서 Numpy, Pytorch의 CPU의 랜덤 생성기 시드, 내장 모듈 random의 랜덤 넘버 생성기, 파이썬의 해시 시드를 설정하였다. 
<br>dataloader = torch.utils.data.DataLoader(
&nbsp; &nbsp; datasets.MNIST('./data/mnist', train=True, download=True,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;transform=transforms.Compose([
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;transforms.Resize(28),
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;transforms.ToTensor(),
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;transforms.Normalize([0.5], [0.5]) &nbsp;# Normalize to range [-1, 1]
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;])),
&nbsp; &nbsp; batch_size=64, shuffle=True)

복사<br>
<br>mnist라는 파이토치에서 제공하는 데이터셋을 사용하였다. compose는 여러 변환을 연속적으로 적용하고 Resize는 이미지를 28 * 28사이즈로 조정하고 ToTensor는 이미지를 파이토치 텐서로 변환한다. 또한 Normalize는 이미지의 픽셀 값을 정규화하여 [-1, 1] 범위로 변환한다. 아래는 전체 중 64개의 샘플을 무작위로 섞어서 불러온다는 뜻이다.
<br>import torch
import torch.nn as nn

class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(28*28, 1024),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            nn.Linear(1024, 512),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            nn.Linear(256, 1),
            nn.Sigmoid()  # Sigmoid activation to output probabilities
        )

    def forward(self, img):
        img_flat = img.view(img.size(0), -1)
        return self.model(img_flat)
복사<br>
<br>판별자를 정의하는 코드이다. 입력된 이미지가 실제 데이터인지 생성된 데이터인지를 구분하는 역할을 한다. 먼저 클래스를 만들고 입력 이미지를 평탄화하고 leaky ReLU 활성화 ㅎ마수로 음수 영역에 작은 기울기를 주고 과적합을 방지하고 노드를 줄여 최종적으로 1개의 출력으로 줄이는 선형 레이어를 만든 후 출력값을 [0,1] 범위로 변환하여 확률로 계산할 수 있게 한다. forward부분은 입력 이미지를 평탄화여 모델에서 출력값을 얻도록 하는 코드이다. 
<br># Loss function
criterion = nn.BCELoss()

# Number of epochs
num_epochs = 100

# For visualizing the progress
fixed_noise = torch.randn(64, 100, device=device)

os.makedirs('./images', exist_ok=True)
os.makedirs('./results', exist_ok=True)

# Training loop
for epoch in range(num_epochs):
    for i, (real_images, _) in enumerate(dataloader):
        if epoch == 0 and i == 0:
            sample_real_images = real_images # to keep batch_size

        batch_size = real_images.size(0)

        # Real labels are 1, fake labels are 0
        real_labels = torch.ones(batch_size, 1).to(device)
        fake_labels = torch.zeros(batch_size, 1).to(device)

        # Train Discriminator with real images
        discriminator.zero_grad()
        outputs = discriminator(real_images.to(device))
        d_loss_real = criterion(outputs, real_labels)
        real_score = outputs

        # Train Discriminator with fake images
        noise = torch.randn(batch_size, 100, device=device)
        fake_images = generator(noise)
        outputs = discriminator(fake_images.detach())
        d_loss_fake = criterion(outputs, fake_labels)
        fake_score = outputs

        # Backprop and optimize for discriminator
        d_loss = d_loss_real + d_loss_fake
        d_loss.backward()
        optimizer_D.step()

        # Train Generator
        generator.zero_grad()
        outputs = discriminator(fake_images)
        g_loss = criterion(outputs, real_labels)

        # Backprop and optimize for generator
        g_loss.backward()
        optimizer_G.step()

        if (i+1) % 200 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}, D(x): {real_score.mean().item():.2f}, D(G(z)): {fake_score.mean().item():.2f}')

    # Save real images once for comparison
    if epoch == 0:
        save_image(sample_real_images, './images/real_images.png')

    # Save sampled images every epoch
    fake_images = generator(fixed_noise)
    save_image(fake_images, f'./images/fake_images_epoch_{epoch+1:04d}.png')

    # Save discriminator results for each image in the grid
    discriminator_results = discriminator(fake_images).cpu().detach().numpy().reshape(8, 8)
    np.save(f'./results/discriminator_outputs_epoch_{epoch+1:04d}.npy', discriminator_results)

print('Training finished.')
복사<br>
<br>
이는 간단히 모델을 학습시킬 총 에포크 수를 정하고 진짜 이미지를 사용해 판별자를 학습한 후 가짜 이미지를 다시 학습하고 생성자는 판별자를 속이기 위해 진짜처럼 보이는 이미지를 생성하도록 학습된다. 

<br>
이후 학습된 생성자 이미지를 생성하고 시각화하면 얼마나 실제 이미지와 비슷한지 확인할 수 있다. 

<br>
이 외에 타 코드를 이용해 계란을 학습하고 나온 결과물이다.<br>
<img alt="egg_.png" src="lib\media\egg_.png">

<br><br>
<br>코드를 실행시켜보는데 정말 원하는 대로 결과가 나오지 않아 답답하고 어디를 어떻게 고쳐야 하는지 감이 잘 오지 않았다. 만들어본 생성 이미지는 꿈에 나올까 무서울 수준이고 계란 이미지를 사용했는데 어디를 봐야 계란이 나오는지 모르겠다. 절망스러웠지만 어디를 고쳐야하는지부터 알아보기로 했다.
]]></description><link>모각코\모각코-5회차-개인-목표-및-공부결과.html</link><guid isPermaLink="false">모각코/모각코 5회차 개인 목표 및 공부결과.md</guid><pubDate>Thu, 08 Aug 2024 07:21:49 GMT</pubDate><enclosure url="lib\media\egg_.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\egg_.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[모각코 6회차 개인 목표 및 공부결과]]></title><description><![CDATA[ 
 <br><br>
<br>
]]></description><link>모각코\모각코-6회차-개인-목표-및-공부결과.html</link><guid isPermaLink="false">모각코/모각코 6회차 개인 목표 및 공부결과.md</guid><pubDate>Thu, 08 Aug 2024 07:22:53 GMT</pubDate></item><item><title><![CDATA[모각코 1회차 개인 목표 및 공부 결과]]></title><description><![CDATA[ 
 <br>목표:  데이터 전처리 필요성<br>
결과:<br>
데이터 전처리- 데이터 분석을 위해 반드시 필요한 작업. 이를 하지 않으면 정상적인 분석이 나오지 않을 수 있다. <br>결측치-데이터에 값이 없는 것 혹은 관측되지 않은 것. Null이라 표현한다. 이를 포함하고 분석을 진행하면 오류가 나거나 이상한 분석 결과가 나와 제거하거나 대체해야 한다. 대체할 경우 처리 방법은 다음과 같다.<br>
<br>평균이나 중앙치로 대체 혹은 mode값으로 대체
<br>간단한 예측 모델로 대체
<br>먼저 1번의 경우 데이터가 숫자와 같은 수치형일 경우 평균 혹은 중앙치로 대체하고 범주형(수치로 측정 불가능한 자료 e.g. 성별, 지역, 혈액형 등)일 경우 모드값(가장 많이 관측되는 수)으로 대체된다.<br>
추가로 어떤 데이터인지, 어디서 온 데이터인지 알아두면 데이터 전처리 하는 데 유용하다.<br>
p.s. NA와 Null의 차이점<br>
NA: Not Available<br>
Null: empty object<br>
NaN: not a Number<br>#결측치 부분 메꾸기(viewCount의 평균값으로 바꿈)
test['X'] = test['X'].fillna(test.X.mean())
복사<br>이상치- 데이터셋에서 다른 값들보다 크게 다른 값. 이 또한 분석을 진행하면 이상한 분석 결과가 나올 수 있기 때문에 이 또한 다음과 같은 처리 과정을 거친다. <br>
<br>표준점수로 변환 후 -3이하 및 +3 제거 
<br>IQR 방식
<br>Binning 처리
<br>1번의 방식은 평균이 0, 표준편차가 1인 분포로 변환한 뒤 값이 -3 혹은 +3이상일 때 처리된다.<br>
2번의 방식은 데이터를 4등분 한 다음 그 중 25%와 75%지점의 값의 차이를 극단치로 처리하는 방식이다.<br>
3번은 구간화라고도 하는데 수치형 자료를 범주형으로 바꾸는 작업이다.<br>데이터 분포 변환-말 그대로 데이터의 열(변수)의 분포를 함수 등을 이용해 변환. 데이터를 학습 모델에 넣을 때, 대부분의 모델들은 데이터가 특정 분포를 따를 거라고 가정한다. 따라서 보통 데이터를 Log나 Exp(e^)등의 함수를 이용해 데이터 분포를 변환하게 한다. <br>#log함수 적용
#데이터 'X'의 열에만 반영
df['X_log'] = preprocessing.scale(np.log(df['X']+1)) 
복사<br>데이터 단위 변환- 데이터의 단위를 일정하게 맞추는 작업. 데이터의 단위가 다르면 거리를 기반으로 하는 모델을 사용했을 때 결과가 이상하게 나올 수 있으므로, 단위를 일정하게 맞추는 스케일링이라는 작업을 해야 한다. 많은 통계 분석 방법이 데이터가 종 모양의 분포를 이룬다는 정규성 가정을 기반한다고 하므로 최대한 정규분포로 변환해야 한다.<br>
<br>scaling: 평균이 0, 분산이 1인 분포로 변환
<br>minmax scaling: 특정 범위로 모든 데이터 변환
<br>Box-Cox: 여러 k값 중 가장 작은 sse() 선택
<br>robust_scale: 중앙값, IQR사용
<br>#위의 scaling 적용
df['X_scale'] = preprocessing.scale(df['X']) df['X_minmax_scale'] = preprocessing.MinMaxScaler(df['X'] df['X_boxcox'] = preprocessing.scale(boxcox(df['X']+1)[0]) df['X_robust_scale'] = preprocessing.robust_scale(df['X'])
#데이터 'X'에만 scaling 적용
복사<br>p.s 데이터의 열과 변수는 종종 같은 이름으로 쓰임]]></description><link>모각코\모각코-1회차-개인-목표-및-공부-결과.html</link><guid isPermaLink="false">모각코/모각코 1회차 개인 목표 및 공부 결과.md</guid><pubDate>Sun, 07 Jul 2024 16:04:01 GMT</pubDate></item><item><title><![CDATA[모각코 2회차 개인 목표 및 공부 결과]]></title><description><![CDATA[ 
 <br><br>
<br>음성 데이터 전처리 초급 과정 배우고 실습
<br>후에 있을 머신러닝 모델 실습을 위한 전처리 공부
<br><br>
<br>음성 데이터 전처리

<br>아날로그 음성 신호를 디지털 신호로 바꿔 저장한 데이터이다. 이때 생기는 개념이 샘플링인데 아날로그 음성 신호를 디지털 신호로 변환하는 과정을 말한다. 

<br>나이퀴스트 샘플링 정리: 아날로그 신호의 주파수의 두 배 이상의 샘플링 레이트(연속적인 신호에서 얻어진 단위시간 당 샘플링 횟수) 를 사용해야 신호를 정확히 복원할 수 있음
<br>실습 코드: 음성 데이터 로드, 시각화
<br>librosa 라이브러리(오디오 신호 분석 도구)의 librosa.load 함수 사용, 오디오 데이터를 일관된 형식으맞추기 위해 16kHz로 변경(이를 리샘플링이라 함 리샘플링은 고주파 성분을 제거하고 새로운 샘플 생성)




<br>import librosa  
import matplotlib.pyplot as plt  
import librosa.display  

audio_path = r'C:\Users\김수진\Desktop\AJR - Come Hang Out (Audio).mp3'  
y, sr = librosa.load(audio_path, sr=16000)  

plt.figure(figsize=(10,4))  
librosa.display.waveshow(y, sr=sr)  
plt.title('waveform')  
plt.xlabel('Time (s)')  
plt.ylabel('Amplitude')  
plt.show()

복사<br>
<br>노이즈 제거-소음 줄임(e.g.특정 주파수 범위만 통과시킴)
<br>정규화-음성신호의 진폭을 일정하게 맞추는 과정
<br>MFCC- 음성 신호의 주파수 스펙트럼을 멜 스케일(사람의 청각 인지에 기반한 주파수 스케일)로 변환 후 푸리에 변환을 적용하여 얻는 계수(시간 영역의 신호를 주파수 영역으로 변환하는 수학적 도구)

<br>프레임 분할
<br>프레임 윈도잉-각 프레임에  윈도우 함수 적용
<br>고속 푸리에 변환-각 프레임에 고속 푸리에 변환 적용
<br>멜 필터 뱅크 적용(주파 영역에서 멜 스케일 적용, 여러 개의 삼각형 필터로 신호를 분해하는 과정)-주파수 스펙트럼을 멜 스케일로 변환 
<br>이산 코사인 변환(신호를 압축하는 변환 기법)-로그 멜 스펙트럼에 DCT적용


<br>노이즈 제거, 정규화 예시
<br>y, sr = librosa.load(audio_path, sr=16000)  
y_filtered = librosa.effects.preemphasis(y, coef=0.97)  
y_normalized = y_filtered / np.max(np.abs(y_filtered))  

plt.figure(figsize=(10, 4))  
librosa.display.waveshow(y_normalized, sr=sr)  
plt.title('Filtered and Normalized Waveform')  
plt.xlabel('Time (s)')  
plt.ylabel('Ampliture')  
plt.show()
복사<br>
<br>패딩 및 자르기

<br>음성 데이터의 길이를 조절하는 작업

<br>제로 패딩-짧은 신호에 대해 끝부분에 0을 추가하여 길이를 맞춘다.
<br>자르기-긴 신호는 일정 길이로 잘라서 사용한다. 남은 부분은 무시한다.




<br><br>어떻게 공부해야 할지 막막했으나 음성 데이터가 무엇인지부터 차근차근 알아감. 모르는 기법과 함수들이 많이 나와 어려움을 겪음. 이후 모르는 단어나 함수들은 일단 다 적어놓고 다시 찾아보는 것도 나쁘지 않을 것이라 생각함. ]]></description><link>모각코\모각코-2회차-개인-목표-및-공부-결과.html</link><guid isPermaLink="false">모각코/모각코 2회차 개인 목표 및 공부 결과.md</guid><pubDate>Wed, 31 Jul 2024 08:46:53 GMT</pubDate></item><item><title><![CDATA[모각코 3회차 개인 목표 및 공부 결과]]></title><description><![CDATA[ 
 <br><br>
<br>아나콘다 설치 및 파이토치 설치
<br>파이토치 기초 잡기
<br><br>
<br>
아나콘다란?

<br>다양한 패키지 관리와 환경 관리 기능을 제공하는 오픈소스 배포판이다. 이를 사용하면 가상환경을 쉽게 만들고 관리할 수 있다.  venv보다는 좀 더 큰 툴로, 다양한 언어를 지원하며 가상환경을 venv와 달리 보다 다양하게 다룰 수 있고, 기존 환경을 복제하여 다른 프로젝트에 사용할 수 있다.
<br>아나콘다 홈페이지에 들어가서 설치할 수 있다.


<br>
파이토치란?

<br>신경망 구축에 사용되는 오픈 소스 딥러닝 프레임워크. 다양한 신경망 아키텍처를 지원한다. 전체 코드를 구현할 때까지 기다리지 않고 실시간으로 코드의 일부를 실행하고 테스트 할 수 있다는 강점이 있다. 


<br>
파이토치 설치 

<br>아나콘다 설치 후 IntelliJ 터미널에서  아래의 코드를 입력하면 된다


<br>conda install pytorch torchvision torchaudio -c pytorch
복사<br><br>
<br>텐서

<br>텐서 ; 배열이나 행렬과 매우 유사한 특수한 자료구조이다. 텐서를 사용해 모델의 입력과 출력, 모델의 매개변수들을 부호화(정보 처리 과정의 한 형태. 컴퓨터를 이용해 영상, 이미지, 소리 데이터를 생성할 때 데이터의 양을 줄이기 위해 데이터를 코드화하고 압축하는 것) ndarray와 매우 유사하며 실제로 numpy배열에서 텐서로 변환이 가능하다.
<br>일반적인 수식으로도 텐서를 결합할 수 있지만 torch.stack이라는 텐서 결합 연산자가 있어 이를 이용할 수도 있다. 


<br><br>pytorch에 대한 기본 개념이 없어 처음 개념이해부터 힘들어서 용어정리부터 하면서 공부해야겠다고 생각함. 실습을 어떻게 해야되나 막막하게 느껴짐. 실습은 좀 미루거나 파이토치에 대해 좀 더 알아보고 실습해야함을 깨달음.]]></description><link>모각코\모각코-3회차-개인-목표-및-공부-결과.html</link><guid isPermaLink="false">모각코/모각코 3회차 개인 목표 및 공부 결과.md</guid><pubDate>Wed, 31 Jul 2024 07:44:04 GMT</pubDate></item><item><title><![CDATA[모각코 4회차 개인 목표 및 공부결과]]></title><description><![CDATA[ 
 <br><br>
<br>파이토치 학습

<br>dataset&amp;dataloader
<br>transform


<br><br><br>
<br>데이터셋 코드와 학습 코드는 분리하는 게 좋다(유지보수가 어려울 수 있기 때문)
<br>Pytorch의 FashionMNIST와 같은 다양한 데이터셋을 제공한다. 
<br>from torch.utils.data import Dataset  
from torchvision.transforms import ToTensor  
from torchvision import datasets  
import matplotlib.pyplot as plt

training_data = datasets.FashionMNIST(  
    root="data",  
    train=True,  
    download=True,  
    transform=ToTensor()  
)

test_data = datasets.FashionMNIST(  
    root="data",  
    train=False,  
    download=True,  
    transform=ToTensor()  
)
복사<br>
<br>첫 번째 단락의 코드는 필요한 모듈을 임포트 하는 과정이고 두번째 단락은 FashionMNIST 데이터셋을 로딩하고 pytorch에서 사용할 수 있도록 텐서 형태로 변환하여 training_data변수에 저장하는 역할이다. 마지막 단락은 FashionMNIST데이터셋의 테스트 데이터를 로딩하고 이를 test_data변수에 저장하는 역할을 한다. 
<br>개인적으로 실습을 진행하다 세 번째 단락의 train =True라고 적는 실수를 저질렀는데, False라고 적어야하는 이유는 Ture라 적으면 학습 데이터가 들어오게 되어 과적합이 일어날 수 있기 때문이라고 한다. 모델이 학습한 데이터를 학습한 결과를 평가할 때 사용하는 데이터인 테스트 데이터로 사용하게 되면 객관적인 평가를 할 수 없다는 문제점도 가지고 있다.<br>
<br>아래의 코드를 통해 데이터를 시각화할 수 있다. 이 코드를 실행하면 T-shirt, Trouser 등의 10개의 패션 아이템 중 랜덤으로 9개의 패션 아이템을 뽑는 코드이다.
<br>labels_map = {  
    0: "T-shirt",  
    1: "Trouser",  
    2: "pullover",  
    3: "Dress",  
    4: "Coat",  
    5: "Sandal",  
    6: "shirt",  
    7: "Sneaker",  
    8: "Bag",  
    9: "Ankle Boot",  
}  
figure = plt.figure(figsize=(8, 8))  
cols, rows = 3, 3  
for i in range(1, cols*rows + 1):  
    sample_idx = torch.randint(len(training_data),size=(1,)).item()  
    img, label = training_data[sample_idx]  
    figure.add_subplot(rows, cols, i)  
    plt.title(labels_map[label])  
    plt.axis("off")  
    plt.imshow(img.squeeze(), cmap="gray")  
    plt.show()
복사<br>
<br>dataloader는 샘플들을 미니배치(전체 데이터를 N등분하여 각각의 학습 데이터를 배치 방식((한번에 큰 묶음으로 데이터를 입력받는것))으로 학습) 하고 매 에폭(인공신경망에서 전체 데이터셋에 대한 한번 학습을 완료한 상태)마다 데이터를 섞어 과적합을 막고 멀티프로세싱(다수의 프로세서가 다수의 작업을 함께 처리하는 것)을 사용해 검색속도를 높이려고 하는 모든 과정을 간단한 API(소프트웨어 애플리케이션이 서로 통신하여 데이터, 특징 및 기능을 교환할 수 있도록 한 일련의 규칙)로 이러한 복잡한 과정들을 추상화한 객체이다. (iterable하다)
<br>from torch.utils.data import DataLoader

train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)
test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)
복사<br><br>
<br>이 작업은 데이터를 조작하고 학습에 적합하게 만든다.  
<br>TorchVision 데이터셋들은 데이터를 변형할 수 있도록 두 개의 주요 매개변수인 transform과  target_transform을 제공한다. transform은 feature(특징- 예를 들어 이미지 데이터라면 크기 조정, 정규화, 텐서 등이 있음)을 변경하기 위한 매개변수로 입력 데이터를 학습에 적합한 형태로 변형한다. target_transform은 레이블(정답)을 변경하기 위한 매개변수이다. 레이블을 원-핫 인코딩(원-핫 인코딩: 범주형 데이터를 이진 벡터로 변환하는 방법 예를 들어 고양이와 개가 있다고 하면 고양이는 (1, 0), 개는 (0, 1)으로 둘 수 있다.)이나 라벨 스무딩( 라벨 스무딩: 원-핫 인코딩과 비슷한 방법이나 좀 더 자세하다. 예를 들어 원-핫 인코딩에서 고양이는 (1, 0), 개는 (0, 1)이였다면 라벨 스무딩을 이용하면 고양이는 (0.9, 0.1), 개는(0.1, 0.9)로 표현할 수 있다. ) 등의 변형을 지원한다.
<br><br>실습에서 오류가 난 줄 모르고 그대로 진행했다가 보고 따라한 코드와 달라 비교해봤을 때 틀렸다는 걸 인지하자 궁금증이 생겼다. 왜 오류가 나지 않았지? 검색해보고 내가 이 코드를 잘못 이해하고 있었음과 train을 어떨 때 True를 써야 하는지 알게 되었다. 이 경험을 통해 여태까지 내가 실습할 때 코드를 잘 이해하지 않고 그냥 따라만 쓰지는 않았나 성찰하는 시간을 가졌다.]]></description><link>모각코\모각코-4회차-개인-목표-및-공부결과.html</link><guid isPermaLink="false">모각코/모각코 4회차 개인 목표 및 공부결과.md</guid><pubDate>Mon, 05 Aug 2024 06:59:08 GMT</pubDate></item><item><title><![CDATA[index]]></title><description><![CDATA[ 
 <br>]]></description><link>index.html</link><guid isPermaLink="false">index.md</guid><pubDate>Wed, 26 Jun 2024 13:06:53 GMT</pubDate></item></channel></rss>